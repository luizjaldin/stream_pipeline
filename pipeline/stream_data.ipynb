{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pyspark==3.4.0 in /Users/luiz.oliveira/projects/n_projects/lib/python3.7/site-packages (3.4.0)\n",
      "Requirement already satisfied: py4j==0.10.9.7 in /Users/luiz.oliveira/projects/n_projects/lib/python3.7/site-packages (from pyspark==3.4.0) (0.10.9.7)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip available: \u001b[0m\u001b[31;49m22.2.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.3.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install pyspark==3.4.0\n",
    "bin/kafka-console-consumer --bootstrap-server localhost:9092 --topic topic_streaming_data --from-beginning\n",
    "bin/kafka-console-consumer.sh --topic topic_streaming_data --bootstrap-server localhost:9092 --from-beginning\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql import streaming\n",
    "import datetime\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The operation couldn’t be completed. Unable to locate a Java Runtime.\n",
      "Please visit http://www.java.com for information on installing Java.\n",
      "\n",
      "/Users/luiz.oliveira/projects/n_projects/lib/python3.7/site-packages/pyspark/bin/spark-class: line 97: CMD: bad array subscript\n",
      "head: illegal line count -- -1\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Java gateway process exited before sending its port number",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/xl/09d4tdwj5s9cs2d6747cz4540000gp/T/ipykernel_34933/2743706525.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mspark\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSparkSession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuilder\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0;34m.\u001b[0m\u001b[0mappName\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"StreamingPipeline\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0;34m.\u001b[0m\u001b[0mmaster\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"local[2]\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0;34m.\u001b[0m\u001b[0mgetOrCreate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/projects/n_projects/lib/python3.7/site-packages/pyspark/sql/session.py\u001b[0m in \u001b[0;36mgetOrCreate\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    475\u001b[0m                         \u001b[0msparkConf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    476\u001b[0m                     \u001b[0;31m# This SparkContext may be an existing one.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 477\u001b[0;31m                     \u001b[0msc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSparkContext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetOrCreate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msparkConf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    478\u001b[0m                     \u001b[0;31m# Do not update `SparkConf` for existing `SparkContext`, as it's shared\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    479\u001b[0m                     \u001b[0;31m# by all sessions.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/projects/n_projects/lib/python3.7/site-packages/pyspark/context.py\u001b[0m in \u001b[0;36mgetOrCreate\u001b[0;34m(cls, conf)\u001b[0m\n\u001b[1;32m    510\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mSparkContext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    511\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mSparkContext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_active_spark_context\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 512\u001b[0;31m                 \u001b[0mSparkContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconf\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconf\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mSparkConf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    513\u001b[0m             \u001b[0;32massert\u001b[0m \u001b[0mSparkContext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_active_spark_context\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    514\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mSparkContext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_active_spark_context\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/projects/n_projects/lib/python3.7/site-packages/pyspark/context.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, master, appName, sparkHome, pyFiles, environment, batchSize, serializer, conf, gateway, jsc, profiler_cls, udf_profiler_cls, memory_profiler_cls)\u001b[0m\n\u001b[1;32m    196\u001b[0m             )\n\u001b[1;32m    197\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 198\u001b[0;31m         \u001b[0mSparkContext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_ensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgateway\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mgateway\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconf\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    199\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    200\u001b[0m             self._do_init(\n",
      "\u001b[0;32m~/projects/n_projects/lib/python3.7/site-packages/pyspark/context.py\u001b[0m in \u001b[0;36m_ensure_initialized\u001b[0;34m(cls, instance, gateway, conf)\u001b[0m\n\u001b[1;32m    430\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mSparkContext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    431\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mSparkContext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_gateway\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 432\u001b[0;31m                 \u001b[0mSparkContext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_gateway\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgateway\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mlaunch_gateway\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    433\u001b[0m                 \u001b[0mSparkContext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jvm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSparkContext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_gateway\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjvm\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    434\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/projects/n_projects/lib/python3.7/site-packages/pyspark/java_gateway.py\u001b[0m in \u001b[0;36mlaunch_gateway\u001b[0;34m(conf, popen_kwargs)\u001b[0m\n\u001b[1;32m    104\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misfile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconn_info_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 106\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Java gateway process exited before sending its port number\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    107\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    108\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconn_info_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"rb\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0minfo\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Java gateway process exited before sending its port number"
     ]
    }
   ],
   "source": [
    "spark = SparkSession.builder \\\n",
    "    .config(\"spark.driver.host\", \"localhost\") \\\n",
    "    .config(\"spark.jars.packages\", \"org.apache.spark:spark-sql-kafka-0-10_2.12:3.4.0\") \\\n",
    "    .appName(\"StreamingPipeline\") \\\n",
    "    .master(\"local[2]\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data_source = \"path/to/your/json/data\"\n",
    "#data_source = {x:x for x in range(1000)}\n",
    "\n",
    "kafka_bootstrap_servers = 'localhost:9092'\n",
    "kafka_topic = 'topic_streaming_data'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "streaming_df = spark.readStream \\\n",
    "    .format(\"kafka\") \\\n",
    "    .option(\"kafka.bootstrap.servers\", kafka_bootstrap_servers) \\\n",
    "    .option(\"subscribe\", kafka_topic) \\\n",
    "    .load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- key: binary (nullable = true)\n",
      " |-- value: binary (nullable = true)\n",
      " |-- topic: string (nullable = true)\n",
      " |-- partition: integer (nullable = true)\n",
      " |-- offset: long (nullable = true)\n",
      " |-- timestamp: timestamp (nullable = true)\n",
      " |-- timestampType: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "streaming_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "streaming_df = streaming_df.withColumn(\"year\", F.year(\"timestamp\"))\n",
    "streaming_df = streaming_df.withColumn(\"month\",F.month(\"timestamp\"))\n",
    "streaming_df = streaming_df.withColumn(\"day\", F.dayofmonth(\"timestamp\"))\n",
    "streaming_df = streaming_df.withColumn(\"hour\", F.hour(\"timestamp\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_path = os.getcwd().split('stream_pipeline')[0] + \"stream_pipeline/output/\" + str(datetime.datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"))\n",
    "checkpoint_path = os.getcwd().split('stream_pipeline')[0] + \"stream_pipeline/checkpoint_path\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/01/03 21:44:50 WARN ResolveWriteToStream: spark.sql.adaptive.enabled is not supported in streaming DataFrames/Datasets and will be disabled.\n",
      "24/01/03 21:44:50 WARN StreamingQueryManager: Stopping existing streaming query [id=547f62bc-88d2-4b31-9c5a-1adba662a352, runId=efa1da5f-ee1c-45a7-818e-456a4ace3687], as a new run is being started.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/01/03 21:44:50 WARN AdminClientConfig: These configurations '[key.deserializer, value.deserializer, enable.auto.commit, max.poll.records, auto.offset.reset]' were supplied but are not used yet.\n"
     ]
    }
   ],
   "source": [
    "query = streaming_df.writeStream \\\n",
    "    .outputMode(\"append\") \\\n",
    "    .format(\"parquet\") \\\n",
    "    .option(\"path\", output_path) \\\n",
    "    .option(\"checkpointLocation\", checkpoint_path) \\\n",
    "    .partitionBy(\"year\", \"month\", \"day\", \"hour\") \\\n",
    "    .trigger(processingTime='1 minute') \\\n",
    "    .start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'message': 'Waiting for next trigger', 'isDataAvailable': False, 'isTriggerActive': False}\n"
     ]
    }
   ],
   "source": [
    "print(query.status)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.parquet(os.getcwd().split('stream_pipeline')[0] + \"stream_pipeline/output\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.withColumn(\"value_2\", F.decode(F.col(\"value\"), 'UTF-8'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-------------------------------------------------------------------+--------------------+---------+------+-----------------------+-------------+----+-----+---+----+----------------------+\n",
      "|key |value                                                              |topic               |partition|offset|timestamp              |timestampType|year|month|day|hour|value_2               |\n",
      "+----+-------------------------------------------------------------------+--------------------+---------+------+-----------------------+-------------+----+-----+---+----+----------------------+\n",
      "|null|[4D 65 6E 73 61 67 65 6D 20 64 65 20 65 78 65 6D 70 6C 6F 20 30]   |topic_streaming_data|0        |442   |2024-01-03 21:44:44.278|0            |2024|1    |3  |21  |Mensagem de exemplo 0 |\n",
      "|null|[4D 65 6E 73 61 67 65 6D 20 64 65 20 65 78 65 6D 70 6C 6F 20 31]   |topic_streaming_data|0        |443   |2024-01-03 21:44:44.28 |0            |2024|1    |3  |21  |Mensagem de exemplo 1 |\n",
      "|null|[4D 65 6E 73 61 67 65 6D 20 64 65 20 65 78 65 6D 70 6C 6F 20 32]   |topic_streaming_data|0        |444   |2024-01-03 21:44:44.282|0            |2024|1    |3  |21  |Mensagem de exemplo 2 |\n",
      "|null|[4D 65 6E 73 61 67 65 6D 20 64 65 20 65 78 65 6D 70 6C 6F 20 33]   |topic_streaming_data|0        |445   |2024-01-03 21:44:44.283|0            |2024|1    |3  |21  |Mensagem de exemplo 3 |\n",
      "|null|[4D 65 6E 73 61 67 65 6D 20 64 65 20 65 78 65 6D 70 6C 6F 20 34]   |topic_streaming_data|0        |446   |2024-01-03 21:44:44.284|0            |2024|1    |3  |21  |Mensagem de exemplo 4 |\n",
      "|null|[4D 65 6E 73 61 67 65 6D 20 64 65 20 65 78 65 6D 70 6C 6F 20 35]   |topic_streaming_data|0        |447   |2024-01-03 21:44:44.285|0            |2024|1    |3  |21  |Mensagem de exemplo 5 |\n",
      "|null|[4D 65 6E 73 61 67 65 6D 20 64 65 20 65 78 65 6D 70 6C 6F 20 36]   |topic_streaming_data|0        |448   |2024-01-03 21:44:44.286|0            |2024|1    |3  |21  |Mensagem de exemplo 6 |\n",
      "|null|[4D 65 6E 73 61 67 65 6D 20 64 65 20 65 78 65 6D 70 6C 6F 20 37]   |topic_streaming_data|0        |449   |2024-01-03 21:44:44.286|0            |2024|1    |3  |21  |Mensagem de exemplo 7 |\n",
      "|null|[4D 65 6E 73 61 67 65 6D 20 64 65 20 65 78 65 6D 70 6C 6F 20 38]   |topic_streaming_data|0        |450   |2024-01-03 21:44:44.288|0            |2024|1    |3  |21  |Mensagem de exemplo 8 |\n",
      "|null|[4D 65 6E 73 61 67 65 6D 20 64 65 20 65 78 65 6D 70 6C 6F 20 39]   |topic_streaming_data|0        |451   |2024-01-03 21:44:44.288|0            |2024|1    |3  |21  |Mensagem de exemplo 9 |\n",
      "|null|[4D 65 6E 73 61 67 65 6D 20 64 65 20 65 78 65 6D 70 6C 6F 20 31 30]|topic_streaming_data|0        |452   |2024-01-03 21:44:44.289|0            |2024|1    |3  |21  |Mensagem de exemplo 10|\n",
      "|null|[4D 65 6E 73 61 67 65 6D 20 64 65 20 65 78 65 6D 70 6C 6F 20 31 31]|topic_streaming_data|0        |453   |2024-01-03 21:44:44.291|0            |2024|1    |3  |21  |Mensagem de exemplo 11|\n",
      "|null|[4D 65 6E 73 61 67 65 6D 20 64 65 20 65 78 65 6D 70 6C 6F 20 31 32]|topic_streaming_data|0        |454   |2024-01-03 21:44:44.292|0            |2024|1    |3  |21  |Mensagem de exemplo 12|\n",
      "|null|[4D 65 6E 73 61 67 65 6D 20 64 65 20 65 78 65 6D 70 6C 6F 20 31 33]|topic_streaming_data|0        |455   |2024-01-03 21:44:44.293|0            |2024|1    |3  |21  |Mensagem de exemplo 13|\n",
      "|null|[4D 65 6E 73 61 67 65 6D 20 64 65 20 65 78 65 6D 70 6C 6F 20 31 34]|topic_streaming_data|0        |456   |2024-01-03 21:44:44.294|0            |2024|1    |3  |21  |Mensagem de exemplo 14|\n",
      "|null|[4D 65 6E 73 61 67 65 6D 20 64 65 20 65 78 65 6D 70 6C 6F 20 31 35]|topic_streaming_data|0        |457   |2024-01-03 21:44:44.295|0            |2024|1    |3  |21  |Mensagem de exemplo 15|\n",
      "|null|[4D 65 6E 73 61 67 65 6D 20 64 65 20 65 78 65 6D 70 6C 6F 20 31 36]|topic_streaming_data|0        |458   |2024-01-03 21:44:44.295|0            |2024|1    |3  |21  |Mensagem de exemplo 16|\n",
      "|null|[4D 65 6E 73 61 67 65 6D 20 64 65 20 65 78 65 6D 70 6C 6F 20 31 37]|topic_streaming_data|0        |459   |2024-01-03 21:44:44.296|0            |2024|1    |3  |21  |Mensagem de exemplo 17|\n",
      "|null|[4D 65 6E 73 61 67 65 6D 20 64 65 20 65 78 65 6D 70 6C 6F 20 31 38]|topic_streaming_data|0        |460   |2024-01-03 21:44:44.298|0            |2024|1    |3  |21  |Mensagem de exemplo 18|\n",
      "|null|[4D 65 6E 73 61 67 65 6D 20 64 65 20 65 78 65 6D 70 6C 6F 20 31 39]|topic_streaming_data|0        |461   |2024-01-03 21:44:44.299|0            |2024|1    |3  |21  |Mensagem de exemplo 19|\n",
      "+----+-------------------------------------------------------------------+--------------------+---------+------+-----------------------+-------------+----+-----+---+----+----------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aguardar a conclusão do streaming\n",
    "query.awaitTermination()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "n_projects",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
